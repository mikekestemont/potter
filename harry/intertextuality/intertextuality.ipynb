{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Reuse and Intertextuality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TEXT: Format the HP books and HP movie scripts to the TRACER format. For the movie scripts, this means taking out the names of the speakers. I can give you clear instructions on how to do TRACER formatting (all texts in one text file):\n",
    "```\n",
    "id of seven digits (book 1: 11, book 12) \\t sentence in tokenized version, lowercased tokens, splitted along whitespace \\t \"NULL\" \\t \"book 1, chapter 1\" (free field)\n",
    "1100001 quod sit officium sapientis .   NULL    Summa Contra Gentiles\n",
    "1100002 veritatem meditabitur guttur meum , et labia mea detestabuntur impium .     NULL    Summa Contra Gentiles\n",
    "1100003 prov. 8-7 .     NULL    Summa Contra Gentiles\n",
    "1100004 multitudinis usus , quem in rebus nominandis sequendum philosophus censet , communiter obtinuit ut sapientes dicantur qui res directe ordinant et eas bene gubernant .  NULL    Summa Contra Gentiles\n",
    "```\n",
    "\n",
    "2. SYNONYMS: For near-verbatim/paraphrase detection, you need an English list of synonyms or thesaurus. This can be extracted from Wordnet and also needs to be formatted as a bidirectional list in two columns. I can also give you details for this. Problem is: many HP neologisms won't be in a standard thesaurus. This gives us two options:\n",
    "(If we don't do this, we will only superexact matches.)\n",
    "    * exhaustive list of synonyms but only for words in movie scripts or books; also add british vs american list\n",
    "    * has to be \"directional\". E.g.\n",
    "\n",
    "```\n",
    "- love \\t care\n",
    "- care \\t love\n",
    "```\n",
    "3. LEMMAS: PoS-tag and lemmatise the corpus. Here I ask that you do it with StanfordCore NLP as the output is recognised by TRACER and no extra conversion is needed.\n",
    "```\n",
    "lowercased wordform \\t baseform/lemma from corenlp \\t postag \\n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to use same tokenizer for both data streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "import spacy\n",
    "nlp = spacy.load('en') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First subtitles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-Harry_Potter_and_the_Sorcerer_s_Stone\n",
      "02-Harry_Potter_and_the_Chamber_of_Secrets\n",
      "03-Harry_Potter_and_the_Prisoner_of_Azkaban\n",
      "04-Harry_Potter_And_The_Goblet_Of_Fire\n",
      "05-Harry_Potter_and_the_Order_of_the_Phoenix\n",
      "06-Harry_Potter_And_The_Half_blood_Prince\n",
      "07a-Harry_Potter_and_Deathly_Hallows_Part_1\n",
      "07b-Harry_Potter_And_Deathly_Hallows_Part_2\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pysrt\n",
    "\n",
    "title_cnt = 10\n",
    "\n",
    "lemma_pos = set()\n",
    "\n",
    "with open('lines.txt', 'w') as new_file:\n",
    "    filenames = sorted(glob.glob('/Users/mike/GitRepos/potter/data/subtitles/*.srt'))\n",
    "    \n",
    "    for filename in filenames:\n",
    "        title_cnt += 1\n",
    "        title = os.path.basename(filename).split('.')[0]\n",
    "        title = title.split('(')[0].strip().replace(' ', '_')\n",
    "        print(title)\n",
    "        \n",
    "        sub_cnt = 0\n",
    "        for sub in pysrt.open(filename):\n",
    "            sub_cnt += 1\n",
    "            \n",
    "            start_time = sub.end.to_time().strftime('%H:%M:%S')\n",
    "            end_time = sub.end.to_time().strftime('%H:%M:%S')\n",
    "            info = title + '-' + start_time + '-' + end_time\n",
    "            \n",
    "            text = ' '.join(sub.text_without_tags.split())\n",
    "            tokens = [t.lower() for t in word_tokenize(text)]\n",
    "            \n",
    "            c = str(sub_cnt)\n",
    "            while len(c) < 6:\n",
    "                c = '0' + c\n",
    "\n",
    "            new_file.write(str(title_cnt) + c + '\\t')\n",
    "            \n",
    "            for token in nlp(text):\n",
    "                form = ''.join([c for c in token.text.lower() if c.isalpha()])\n",
    "                if form:\n",
    "                    new_file.write(form + ' ')\n",
    "                    lemma = token.lemma_.lower()\n",
    "                    pos = token.tag_\n",
    "                    lemma_pos.add(tuple([form, lemma, pos]))\n",
    "            \n",
    "            new_file.write('\\tNULL\\t' + info + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then novels (US version for orthographic reasons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import OrderedDict\n",
    "\n",
    "def load_potter(fn):\n",
    "    series = etree.parse(fn)\n",
    "    HP = OrderedDict()\n",
    "    for book in series.iterfind('.//book'):\n",
    "        book_title = book.attrib['title']\n",
    "        #print(book_title)\n",
    "        HP[book_title] = OrderedDict()\n",
    "        \n",
    "        for chapter in book.iterfind('.//chapter'):\n",
    "            chapter_title = chapter.attrib['title']\n",
    "            #print('   ', chapter_title)\n",
    "            HP[book_title][chapter_title] = []\n",
    "            \n",
    "            for paragraph in chapter.iterfind('.//p'):\n",
    "                text = ''.join([x for x in paragraph.itertext()])\n",
    "                HP[book_title][chapter_title].append(text)\n",
    "    return HP\n",
    "\n",
    "novels = load_potter('../preprocessing/simple_potter_us.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "Harry Potter and the Chamber of Secrets\n",
      "Harry Potter and the Prisoner of Azkaban\n",
      "Harry Potter and the Goblet of Fire\n",
      "Harry Potter and the Order of the Phoenix\n",
      "Harry Potter and the Half-Blood Prince\n",
      "Harry Potter and the Deathly Hallows\n"
     ]
    }
   ],
   "source": [
    "with open('lines.txt', 'a') as new_file:\n",
    "    for book in novels:\n",
    "        print(book)\n",
    "        title_cnt += 1\n",
    "        for chapter in novels[book]:\n",
    "            sent_cnt = 0\n",
    "            for paragraph in novels[book][chapter]:\n",
    "                for sentence in sent_tokenize(paragraph):\n",
    "                    sent_cnt += 1\n",
    "                    text = sentence.strip()\n",
    "                    tokens = word_tokenize(text)\n",
    "                    tokens = [t.lower() for t in tokens]\n",
    "                    info = book.replace(' ', '_') + '-' + chapter.replace(' ', '_')\n",
    "                    c = str(sent_cnt)\n",
    "                    while len(c) < 6:\n",
    "                        c = '0' + c\n",
    "                    \n",
    "                    new_file.write(str(title_cnt) + c + '\\t')\n",
    "\n",
    "                    for token in nlp(text):\n",
    "                        form = ''.join([c for c in token.text.lower() if c.isalpha()])\n",
    "                        if form:\n",
    "                            new_file.write(form + ' ')\n",
    "                            lemma = token.lemma_.lower()\n",
    "                            pos = token.tag_\n",
    "                            lemma_pos.add(tuple([form, lemma, pos]))\n",
    "\n",
    "                    new_file.write('\\tNULL\\t' + info + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump lemmas and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lemma_pos.txt', 'w') as new_file:\n",
    "    for item in lemma_pos:\n",
    "        new_file.write('\\t'.join(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "with open('synonyms.txt', 'w') as f:\n",
    "    for token, lemma, pos in lemma_pos:\n",
    "        for synset in wn.synsets(lemma):\n",
    "            for synonym in synset.lemma_names():\n",
    "                if synonym.lower() != lemma:\n",
    "                    lemma = lemma.replace('_', ' ').lower()\n",
    "                    synonym = synonym.replace('_', ' ').lower()\n",
    "                    \n",
    "                    f.write('\\t'.join((lemma, synonym)) + '\\n')\n",
    "                    f.write('\\t'.join((synonym, lemma)) + '\\n')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Add UK-US couples?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.stem import WordNetLemmatizer\n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#with open('synonyms.txt', 'a') as f:\n",
    "#    for line in open('../collation/uk_vs_us.txt', 'r'):\n",
    "#        if line.startswith('#'):\n",
    "#            continue\n",
    "        \n",
    "#        a, b = line.strip().split()\n",
    "#        a = wordnet_lemmatizer.lemmatize(a)\n",
    "#        b = wordnet_lemmatizer.lemmatize(b)\n",
    "        \n",
    "#        f.write('\\t'.join((a, b)) + '\\n')\n",
    "#        f.write('\\t'.join((b, a)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clean36]",
   "language": "python",
   "name": "conda-env-clean36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
