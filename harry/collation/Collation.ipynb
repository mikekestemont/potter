{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hogwarts High\n",
    "## Collating the British and American versions of Harry Potter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "It is a well-known fact that the Harry Potter novels initially appeared in a so-called \"British version\" (UK), which was subsequently edited into an \"American version\" (VS). There are at least two intriguing aspects to this:\n",
    "\n",
    "- Rowling herself worked **herself**, with her American editor, on the US version; thus, she can be said to have endorsed or even \"authorized\" the changes. Because of this, one could easily claim that the US version should not be considered a mere derivative of a \"more original, authentic\" UK version.\n",
    "- The changes are relatively **frequent** and sometimes fairly far-reaching: they involve much more than just superficial modifications, e.g. on the level of orthography. Especially, the many (seemingly unnecessary) stylistic interventions are striking (see below) and they are not always easy to account for on the basis of straightforward cultural differences.\n",
    "\n",
    "One extremely complicating matter is that the differences between the US and UK versions are not the only differences  that have to be taken into account. Throughout the years, Rowling and her publishers made numerous adaptations to the text, for instance, emendations that corrected obvious mistakes in the plot (consult e.g. [this semi-official list](https://www.hp-lexicon.org/differences-changes-text/)). Although Bloomsbury, her UK publishing house, has published a partial list of the \"official\" changes (by 2004), it is clear that this list is not exhaustive, so that the latest versions contain a curious mix of both **\"canonical\" and \"apocryphal\" variants**. Especially interesting (from the point of view of **stemmatology**, see below), is that some changes were originally made in the US version before making their way into subsequent re-editions of the UK versions. In editorial theory, this phenomenon is known as **contamination**, i.e. the genetic process where (initially independent) lines of textual offspring start to influence or contaminate each other. This should again raise the awareness with the reader that the UK version should necessarily be considered more \"original\" or \"authentic\" with respect to the US version -- one cannot easily claim *precedence* over the other.\n",
    "\n",
    "This makes the study of the Potter variants, official and non-official alike, very convoluted. Powerful **mediating institutions**, like *amazon.com* for instance, moreover make it surprisingly difficult to retrieve specific (e.g. original) versions of the books, let alone in an electronic format. Recent bibliographic studies on Rowling do not help much in this respect and provide no reasonably detailed \"stemma\" or genetic tree of the different versions which have been published.\n",
    "\n",
    "There has been some previous research into the matter, for example, the excellent article by Philip Nel that we use as a basis for this chapter:\n",
    "\n",
    "> Philip Nel, \"You Say 'Jelly,' I Say 'Jell-O'?: Harry Potter and the Transfiguration of Language.\" *The Ivory Tower and Harry Potter: Perspectives on a Literary Phenomenon*, ed. Lana Whited. Columbia and London: University of Missouri Press, pp. 261-84.\n",
    "\n",
    "However, while Nel does a great job at inventorizing and interpreting numerous differences between the US and UK version (especially from the point of view of \"intralingual, but intercultural translation\"), it is clear that he essentially relies on a manual comparison of the books. By necessity, such a laborious exercise must be limited to a small selection or **sample**, compiled during a sustained close reading. For other readers and scholars, it can be hard to assess whether the sample of items is truly representative -- which differences were left out? -- and what the **exact criteria** for inclusion in the discussion were. Moreover, Nel's analysis is qualitative rather than quantitative and considerations of a more empirical nature are notably absent: we do not get to see which books of the series were more heavily edited than others or which *sort* of edits were more common than others.\n",
    "\n",
    "### Collation\n",
    "Here, it is clear that we assume the role of the Devil's advocate in an attempt to introduce the added value which digital methods and distant reading might bring to this debate. We will turn to a popular methodology called **collation** that allows us to provide a higher-level **macroanalysis** of some the deviations between the US and UK version. Collation, in the context of (digital) literary studies, refers to the process where scholars compare and align different versions of the same text, for instance, for the sake of producing an edition with a **critical apparatus**. A prototypical example where collation is a useful is that of a medieval text that survives in a large number of manuscripts or \"witnesses\", which all show (subtle) deviations from each other. In such a case, critics might want to reconstruct the **stemma codicum** of the manuscripts, i.e. establishing a family tree that models which books were in all likelihood copied from each other.\n",
    "\n",
    "Especially in earlier times, this was often done using the so-called **Lachmannian method**, named after its inventor, the nineteenth-century German critic [Karl Lachmann](https://en.wikipedia.org/wiki/Karl_Lachmann). According to this method, it is possible to determine which manuscripts were copied from through each through at the errors which they have in common with respect to the original (i.e. the **Principle of the Common Errors**). While still influential, this view is not entirely uncontested nowadays, partly because it can be difficult for modern readers to determine what exactly constitutes an \"error\".\n",
    "\n",
    "Collation has been -- and continues to be -- an important application in Digital Humanities and the advantages of computers for (semi-)automatic tools were recognized very early in the field. Traditionally, collation was often carried out in a **pairwise fashion**, where the extant copies of a text were aligned with a so-called **base manuscript**, that would serve as the reliable point of reference for an edition. Nowadays, this setup has become less common in textual scholarship. People often try to postpone the choice for a single base text (the *Urtext*) and instead align multiple versions of a text at the same time. Another shift has been that collation previously mostly happened at the level of individual words (often called 'tokens'), whereas modern scholarship increasingly is interested in character-level alignment too.\n",
    "\n",
    "In this chapter we will:\n",
    "1. Load a version of the Harry Potter texts in a handy, nested format\n",
    "2. Look into some very general differences between the US and UK version\n",
    "3. Use the stand-alone Java tool `Collatex` to perform a simple collation of the opening chapter of the series\n",
    "4. (Manually look into the Levenshtein metric, one influential historic method in alignment studies)\n",
    "5. Turn to Python to expand the scope of our collation, using the Python port of Collatex\n",
    "6. Establish a simple \"ontology\" of the sort of edits we find\n",
    "7. Measure whether the profile of the edits has changed throughout the books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Potter series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we parse a ready-made TEI version of the British and American version of the Harry Potter series into two Python objects, `UK` and `US`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from lxml import etree\n",
    "from collections import OrderedDict\n",
    "\n",
    "namespaces = {'tei':'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "def load_potter(indir):\n",
    "    HP = OrderedDict()\n",
    "    for fn in sorted(glob.glob(indir + '/*.xml')):\n",
    "        book = etree.parse(fn)\n",
    "        book_title = book.xpath('//tei:teiHeader//tei:title//text()',\n",
    "                                namespaces=namespaces)[0]\n",
    "        print(book_title)\n",
    "        HP[book_title] = OrderedDict()\n",
    "        \n",
    "        for chapter in book.iterfind('.//tei:div', namespaces=namespaces):\n",
    "            chapter_title = chapter.find('tei:head',\n",
    "                                namespaces=namespaces).text\n",
    "            print('   ', chapter_title)\n",
    "            HP[book_title][chapter_title] = []\n",
    "            \n",
    "            for paragraph in chapter.iterfind('.//tei:p', namespaces=namespaces):\n",
    "                text = ''.join([x for x in paragraph.itertext()])\n",
    "                HP[book_title][chapter_title].append(text)\n",
    "    return HP\n",
    "\n",
    "UK = load_potter('hp_uk_xml')\n",
    "US = load_potter('hp_us_xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UK` and `US` are *nested* Python objects of the type `OrderedDict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(UK))\n",
    "print(type(US))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a special kind of dictionary in which the keys do have an order, unlike the traditional dictionaries in Python. (We still make a big deal out of this here, but starting from Python 3.6 *all* Python dictionaries are in fact (silently) ordered, meaning that they the remember the order in which entries were added to it.) An `OrderedDict` still has keys like a normal `dict` that can be used to retrieve values from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(UK.keys())\n",
    "print(US.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, at the highest tier, `UK` and `US` contain a list of the titles of the individual books in the series in chronological order, so that they can be easily aligned -- i.e. the third title in `UK` corresponds to the third title in `US`. We can now retrieve the type (`type()`) of the value which has been stored for one of these individual titles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goblet = US[\"Harry Potter and the Goblet of Fire\"]\n",
    "print(type(goblet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the next tier in our objects, we see that for each each title, we have stored an additional `OrderedDict`. As can be gleaned from their length and keys, this is where we store the consecutive chapters contained in an book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(goblet))\n",
    "print(goblet.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we drill our way further down this hierarchical structure, we can now inspect what each chapter contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scar = goblet['THE SCAR']\n",
    "print(type(scar))\n",
    "print(len(scar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each chapter is represented by a list, finally, which contains a the list of the \"paragraphs\" -- or rather, text blocks -- in the chapter. We can print a random \"paragraph\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = scar[3]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude: our `UK` and `US` objects offer a multi-tiered or **nested data structure** for the American and British books in de the series, storing for each book title a series of chapters; these chapters in turn contain a list of paragraphs for each chapter. To access one of the paragraphs, you can **stack indexes** on top op each other, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(US['Harry Potter and the Goblet of Fire']['BACK TO THE BURROW'][6])\n",
    "print(UK['Harry Potter and the Deathly Hallows']['Magic is Might'][-1])\n",
    "print(US['Harry Potter and the Chamber of Secrets']['THE VERY SECRET DIARY'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the whole text of a chapter, we simply join the respective paragraphs for that chapter using a whitespace character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chap = ' '.join(UK['Harry Potter and the Deathly Hallows']['Magic is Might'])\n",
    "print(chap[:1000] + '[...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "> Print the last 500 characters of the penultimate chapter in *Harry Potter and the Half-Blood Prince* (US version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your exercise code goes here\n",
    "\n",
    "chap = ' '.join(US[\"Harry Potter and the Half-Blood Prince\"]['THE PHOENIX LAMENT'])\n",
    "print(chap[-500:] + '[...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the Harry Potter texts into our notebooks, we can already make some exploratory, high-level plots of the book series. When looking at the Harry Potter series in a book store, for instance, it is clear that Rowling's books seem to have become longer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://ecx.images-amazon.com/images/I/51BRMauPcNL.jpg \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us warm up a little, through making some simple plots that help us verify this impressions. For this this we make use of an established plotting library in Python, called `matplotlib`, which we import using the **alias** `plt`. To make sure that everything gets displayed correctly, and in a pleasing style, in our notebook, we first need to execute the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.style\n",
    "matplotlib.style.use('seaborn-deep')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to measure the length of a book is to simply count the total number of characters it contains. For the UK books, for instance, we can first loop over the books in `UK`, then over the chapter and then over the paragraphs. For each book, we keep track of the number of characters it contains in a list of integers which we call `uk_len`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_len = []\n",
    "\n",
    "for uk_book in UK:\n",
    "    \n",
    "    uk_cnt = 0\n",
    "    for uk_chap in UK[uk_book]:\n",
    "        uk_text = ' '.join(UK[uk_book][uk_chap])\n",
    "        uk_cnt += len(uk_text)\n",
    "    \n",
    "    uk_len.append(uk_cnt)\n",
    "\n",
    "print(uk_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these numbers, it becomes clear already that the Harry Potter books, at least in the UK version, have grown significantly longer. We can now draw a block to visualize this information. Most of the commands used below should be intuitive enough to make sense to you (e.g. adding a legend and axis labels etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(uk_len, label='uk')\n",
    "plt.ylabel('# characters')\n",
    "plt.xlabel('book titles')\n",
    "plt.xticks(range(len(uk_len)), UK.keys(), rotation='vertical')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple **line plot** visually confirms our expectation: ever since the relatively shorter *Philosopher's Stone*, the books have grown considerably larger. Although the *Half-Blood Prince* somewhat countered this trend, the finale of the series grows longer again - and both concluding volumes are clearly longer than the opening three items in the series. The fifth item, the *Phoenix* reaches the maximum length in characters -- and, as will we see later, this is not the only respect in which the *Phoenix* seems to play an exceptional role in the development of the series. Funnily, Rowling recently tweeted that in her *Cormoran Strike* series too, the \"fourth book\" would be the longest there as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">It progresses. Turns out the fourth in every one of my series has to be the longest. ðŸ™„ <a href=\"https://t.co/cnXl2Qju6Y\">https://t.co/cnXl2Qju6Y</a></p>&mdash; J.K. Rowling (@jk_rowling) <a href=\"https://twitter.com/jk_rowling/status/958376528643002370?ref_src=twsrc%5Etfw\">January 30, 2018</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our numbers, the fourth book in the Harry Potter series does have the largest number of chapters (see below), but it was *not* the longest in terms of length in characters..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literary scholars might easily interpret this development in terms of Rowling's growing success: ever since her books became international bestsellers, she must have obtained more leeway and artistic freedom with her editors, allowing her to create longer books, since the publisher could trust on the success of the upcoming items in the series, so no aggressive truncaiton of the narrative was needed. This is also evident from the number of chapters contained in the book. Using a simple **list comprehension** we retrieve the length of each book in terms of the number of chapters it contains. (For now, you can just think of a list comprehension as a `for` loop that gets written on a single line.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chap_len = [len(UK[t]) for t in UK]\n",
    "print(chap_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting this information requires only minor adjustments to our previous plotting block. Can you see which bits we had to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(chap_len, label='uk')\n",
    "plt.ylabel('# chapters')\n",
    "plt.xlabel('book titles')\n",
    "plt.xticks(range(len(chap_len)), UK.keys(), rotation='vertical')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is interesting how the evolution of the number of chapter per books closely parallels the overall length of the books -- which makes sense intuitively, at least if we assume that the average length of a chapter did not change radically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz\n",
    "> Create the same plots (length of the books in characters and chapters), but now for the US version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "us_len = []\n",
    "\n",
    "for us_book in US:\n",
    "    \n",
    "    us_cnt = 0\n",
    "    for us_chap in US[us_book]:\n",
    "        us_text = ' '.join(US[us_book][us_chap])\n",
    "        us_cnt += len(us_text)\n",
    "    \n",
    "    us_len.append(us_cnt)\n",
    "\n",
    "print(us_len)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(us_len, label='us')\n",
    "plt.ylabel('# chapters')\n",
    "plt.xlabel('book titles')\n",
    "plt.xticks(range(len(chap_len)), US.keys(), rotation='vertical')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Readability** is a thorny issue in the international Rowling criticism, especially in the context of the difference between the UK and US versions. Reviewers, critics and readers alike have noted that many aspects of the US version seem to have been somewhat \"simplified\" in comparison to the UK version. Some critics have gone as far as claiming that this would constitute a pure act of \"dumbing down\" the books for a less intellectual American readership. Others might counter this naive view by arguing that such interventions might actually boost the stylistic qualities of the books. One crude, yet insightful way to quantify such differences would be to inspect the average length of sentences in the books. Would the length of sentences (i.e. the number of word tokens they contain), on average, indeed be shorter in the UK than in the US version?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we need a so-called **sentence splitter**: to a computer, the piece of text contained in our paragraphs is just a series of characters, i.e. a variable of the primitive **string** data type. Outfront, our computer has no idea where a single sentence starts and another might begin. Inspect, for instance, the very first paragraph in the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = UK[\"Harry Potter and the Philosopher's Stone\"]['The Boy Who Lived'][0]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it obvious that we are in fact dealing with two sentences. To calculate the average length of a sentence, we first need to segment these sentences first. To this end we can import the ready-made `sent_tokenize` function that ships with `nltk` (Natural Language Toolkit); this is a language processing library that is commonly used in Python. If we feed this function our paragraph, we can see that it will return a list of strings with the segmented sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = sent_tokenize(p)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we could also split those sentences into strings containing individual words -- or **tokens** as they are more commonly called in **Natural Language Processing** (NLP). Nltk has a simple `word_tokenize` function for exactly that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "first = sentences[0]\n",
    "tokens = word_tokenize(first)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we glue these components together, we are now ready to verify whether the UK and US versions indeed deviate significantly from each other in terms of the average number of words used in a sentence. Using the `zip` function below, we now loop over the books and chapter in the UK and US version simultaneously. (Note, of course, that the number of chapters have never changed in the UK and US version.) Using the `mean` function, from the `statistics` module in Python's **standard library**, we collect for each book its average sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "uk_len, us_len, uk_titles  = [], [], []\n",
    "\n",
    "for uk_book, us_book in zip(UK, US):\n",
    "    print(uk_book, 'vs', us_book)\n",
    "    uk_titles.append(us_book)\n",
    "    \n",
    "    uk_cnt, us_cnt = [], []\n",
    "    for uk_chap, us_chap in zip(UK[uk_book], US[us_book]):\n",
    "        uk_text = ' '.join(UK[uk_book][uk_chap])\n",
    "        us_text = ' '.join(US[us_book][us_chap])\n",
    "        \n",
    "        us_text = us_text.replace('. . .', '.')\n",
    "        \n",
    "        uk_sents = sent_tokenize(uk_text)\n",
    "        us_sents = sent_tokenize(us_text)\n",
    "        \n",
    "        uk_cnt.extend(len(word_tokenize(s)) for s in uk_sents)\n",
    "        us_cnt.extend(len(word_tokenize(s)) for s in us_sents)\n",
    "    \n",
    "    # now average at chapter level\n",
    "    uk_len.append(mean(uk_cnt))\n",
    "    us_len.append(mean(us_cnt))\n",
    "\n",
    "print(uk_len)\n",
    "print(us_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the numbers is very similar to what we have done before, only now, you can see that we call the `plot` function twice, to add a line in the plot for both the `UK` and `US` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(uk_len, label='uk')\n",
    "plt.plot(us_len, label='us')\n",
    "plt.ylabel('average sentence length (in # tokens)')\n",
    "plt.xlabel('# book')\n",
    "plt.xticks(range(len(uk_titles)), uk_titles, rotation='vertical')\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is striking: we can indeed observe that the US sentences *in each book* are a good deal shorter on average than their British counterparts. This difference grows even more pronounced in the later and longer books, where the US editors (and Rowling herself, apparently) seem to have felt an even stronger need to cut down on the average sentence length. While this is a worthwhile observation in itself, we will come back to a more advanced analysis of this phenomenon in the subsequent sections of this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = ' '.join(UK[\"Harry Potter and the Philosopher's Stone\"]['The Boy Who Lived'])\n",
    "for sentence in sent_tokenize(p):\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have been relying on fairly crude and superficial metrics to compare the UK and US versions, e.g. average sentence length. This is indeed typical of much DH/DLS research, where scholars adopt a **distant or macroanalytical perspective** -- \"dumb down, to scale up\", so to speak. In the next section, however, we turn to a methodology that allows us to zoom in again on the text, in order to get a grasp of the *actual* microlevel differences that exist between both versions: (semi-)automatic collation. Let us start with a simple example and collate the first chapters of the UK and US version respectively.\n",
    "\n",
    "For this we can make use of an external Java tool called [Collatex](https://collatex.net/), which is one of the more popular tools which are available for aligning and collating multiple (>=2) versions of the same text. This tool comes as single **jar-file** that you can download from the project's [website](https://collatex.net/) -- we used 1.7.1. (You should place this file in the same folder as this notebook.) Running this jar-file requires that you have a recent version of the **Java** programming language: if you don't (know whether you) have that, it's a good idea to download the correct version for your operating system [here](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use `Collatex` from the command line, which is at this point still the most powerful and flexible interfaces to the software. For this, you can open a new command line window, navigate to the folder where you left the far, and type in the commands that follow below. Alternatively, you can just run the commands in the code blocks below -- through adding an exclamation mark (`!`) we indicate that those commands should be executed in the notebook as if we executed them in the shell. (If you do this from the notebook, however, please be well aware that we are no longer working in simple Python here, but that we switch to shell syntax!)\n",
    "\n",
    "Below, we start by invoking the **help** or **documentation** for the jar-file, which will tell us, amongst others, what sort of input is expected from our side and which settings we can tweak. Some notes on the syntax of this command:\n",
    "- `java` indicates that we want to run the program using the Java programming language\n",
    "- `-jar` is a flag indicating that we run a (fully self-contained) jar-version of the Java program\n",
    "- `collatex-tools-1.7.1.jar` is just the (relative) path to the actual jar file on our system\n",
    "- `-h` is a flag that we use to print out the help documentation that comes with the jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar collatex-tools-1.7.1.jar -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the program with the `-h` flag, returns a list of all the possible flags that we can use and their anticipated behavior or effect on the internal working of the tool -- this includes the documentation `-h` and `--help` flags themselves, as you can see. Most of these flags have so-called `default settings`, which means that, if we are happy with them, we do not have to explicitly set them each time we run the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first dump a simple plain text version of the first chapters in both the UK and US version. In Python, this is as simple as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('uk_chap1.txt', 'w') as f:\n",
    "    text = ' '.join(UK[\"Harry Potter and the Philosopher's Stone\"]['The Boy Who Lived'])\n",
    "    f.write(text)\n",
    "\n",
    "with open('us_chap1.txt', 'w') as f:\n",
    "    text = ' '.join(US[\"Harry Potter and the Sorcerer's Stone\"]['THE BOY WHO LIVED'])\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the contents of these newly created files in your text editor, you should see that they contain simple plain text. To now go and collate these texts, we can just add them as additional arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar collatex-tools-1.7.1.jar uk_chap1.txt us_chap1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should print a rather large string to your console: because we did not specify an output file, the output of the program simply gets passed to the command line (or the so-called **standard output** route) in its entirety. To catch this output and send it to an actual file, there is a again a useful flag for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -jar collatex-tools-1.7.1.jar uk_chap1.txt us_chap1.txt --output chap1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the resulting file in a text editor, you will see that the output in the file `chap1.json` has been formatted as **json**. As you can see, json is a format that allows us to express a number of data collection using a simply structure that is reminiscent of the way lists or dictionaries are used in Python. For now, have a look at the bit that follows `\"table\":`. Can you explain how the result of the collation of two **witnesses** or text versions is reflected in this data structure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Python port"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a couple of years, there exists also a sort of \"rewrite\" or **port** of Collatex to Python and we will now explore that -- the highly respected scholar Ronald Haentjens Dekker (Huygens ING) is the lead developer of the project. Apart from the fact that it interacts really well with Jupyter notebooks, this port comes with a number of intuitive ways to visualize the result of a collation, live in our browser. To install the Python port, executing the following block should suffice -- if you're lucky... Detailed installation instructions can be found [here](http://collatex.obdurodon.org/installation.xhtml), should anything go wrong. As you can see, we also include a **dependency** of Collatex, i.e. another package that `Collatex` relies on for some of its functionality. This dependency should be installed *before* you install Collatex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-levenshtein\n",
    "!pip install Collatex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip` is a tool that allows you to quickly install **third-party packages** that are publicly available (as opposed to the **standard library** that is a part of Python itself). Interestingly, `pip` tries to make sure that you download a version that plays nicely with your specific hardware and software.\n",
    "\n",
    "Let us start with a really obvious but famous a example: the title of the very first Harry Potter book, which was modified, because people thought that the word \"philosopher\" would be to difficult for American children... We add a string representing each title as a witness, and have `Collatex` align those versions. We inspect the result in a so-called alignment table that visualizes the correspondences and deviations between our two witnesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collatex import *\n",
    "\n",
    "collation = Collation()\n",
    "collation.add_plain_witness('UK', \"Harry Potter and the Philosopher's Stone\")\n",
    "collation.add_plain_witness('US', \"Harry Potter and the Sorcerer's Stone\")\n",
    "\n",
    "alignment_table = collate(collation)\n",
    "\n",
    "print(alignment_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Collatex has correctly identified and aligned the parts in which both titles deviate. Essentially, the result of an alignment is a long list of pairs, showing which parts in the first witness (`\"UK\"`) correspond to the second witness which we specified (`\"US\"`). We can iterate over these pairs as follows, if we want to postprocess the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in alignment_table.columns:\n",
    "    for idx, tokens in i.tokens_per_witness.items():\n",
    "        print(tokens)\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another really fancy way to visualize the result of a collation is through the use of a **variant graph**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate(collation, output=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this view, a text is modelled as a **directed, (acyclical) graph** and a witness is viewed as one specific **path** or **traversal** of that graph. This model of texts is becoming increasingly popular -- even \"hipster\" -- in the world of digital scholarly editing and probably for good reasons -- see for instance [this abstract](https://www.balisage.net/Proceedings/vol19/print/Dekker01/BalisageVol19-Dekker01.html). Even the acts of reading and editing a text can be viewed as a traversal of this graph, that allows you to switch or **oscillate** between different witnesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, another highly readable manner to view the results of a collation is through a coloured table. In the next code block, we collate a slightly longer portion of text -- a fairly random segment from the *Prisoner of Azkaban*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_chap = ' '.join(UK[\"Harry Potter and the Prisoner of Azkaban\"]['Owl Post'])[:10000]\n",
    "us_chap = ' '.join(US[\"Harry Potter and the Prisoner of Azkaban\"]['OWL POST'])[:10000]\n",
    "\n",
    "print(uk_chap)\n",
    "print(us_chap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that this will take a while to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collation = Collation()\n",
    "collation.add_plain_witness('UK', uk_chap)\n",
    "collation.add_plain_witness('US', us_chap)\n",
    "collate(collation, layout='vertical', output='html2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In red, this alignment table shows the segments in which the witnesses deviate. At a glance, it becomes obvious that this take on distant reading, however modest, already surfaces interesting differences in a way that would be hard to achieve using traditional means of analysis. Some categories emerge:\n",
    "- The bulk of deviations are fairly straightforward differences in orthography and punctuation that are not extremely interesting from a literary point of view (`neighbours` vs `neighbors`, `toward` versus `towards`);\n",
    "- A second clear categories concerns words that have been replaced by a more American expression (e.g. `torch` versus `flashlight`; `holidays` versus `break`); in many cases, however the semantics of both items is close enough to still speak of (innocent?) **synonyms**;\n",
    "- A third category of changes seem to be **stylistic**. The US version replaces \"stomach\" with \"front\", for instance: in such a case, it becomes harder to speak of pure synonymy, because a front and a stomach are not the exact same thing. The same is also, and perhaps even more true for the clarifying intervention to specify or add that \"Harry pushed his round glasses up *the bridge of* his nose\".\n",
    "\n",
    "The latter category of interventions are much harder to interpret, since we go beyond a simple synonym replacement as the lexical level. Why on earth would one want to change \"the Dursleys\" to, simply, \"they\" in the US version? Or add intensifying adverbs such as `particularly` or `especially` in seemingly random sentences that weren't in dire need for these additions? This category will therefore be a focal aspect of our analysis below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section yielded some proof of concept that collation offers a fruitful methdology to inspect the differences between the UK and US versions at a larger scale. We are now ready to collate the entire series. However, one clear downside of the Python port is that it is much slower to run than the Java tool. In the code blocks below, we therefore first produce collations using the Java tool, which takes \"only\" a couple of hour to run on my laptop, whereas the Python version would have to run overnight. We dump the results as json files that we can load again later for postprocessing in Python. (You can skip the next few code blocks if you are not interested in running the actual collation yourself. We provide the dumps of the analysis so that you can with postprocessing them right away.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def java_alignment(uk_chap, us_chap):\n",
    "    W = {'witnesses': [{'id': 'uk', 'content': uk_chap},\n",
    "                       {'id': 'us', 'content': us_chap}]}\n",
    "    \n",
    "    with open('in.json', 'w') as f:\n",
    "        f.write(json.dumps(W))\n",
    "\n",
    "    subprocess.call(\"java -jar collatex-tools-1.7.1.jar in.json --output='out.json'\", shell=True)\n",
    "    \n",
    "    with open('out.json', 'r') as f:\n",
    "        alignment = json.loads(f.read())\n",
    "    \n",
    "    pairs = list(zip(*alignment['table']))\n",
    "        \n",
    "    uk = pairs[alignment['witnesses'].index('uk')]\n",
    "    us = pairs[alignment['witnesses'].index('us')]\n",
    "\n",
    "    return uk, us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This takes forever.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit java_alignment(uk_chap, us_chap)\n",
    "#%timeit collate(collation, layout='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk, us = java_alignment(uk_chap, us_chap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(uk, us):\n",
    "    print(''.join(a), '<==>', ''.join(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the collation at the chapter level in bulk and dump the results to a list of simple json-files that we can postprocess later - without having to rerun the actual collation over and over again. One important gimmick which we add at this stage is that we tokenize the texts and add a so-called **end-of-sentence marker** (EOS) in between sentences. This will help us study how and where new sentence endings have been introduced in the US version (cf. sentence length)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('collations')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "os.mkdir('collations')\n",
    "\n",
    "cnt = 0\n",
    "for uk_book, us_book in zip(UK, US):\n",
    "    print(uk_book)\n",
    "    for uk_chap, us_chap in zip(UK[uk_book], US[us_book]):\n",
    "        print('   ', uk_chap)\n",
    "        cnt += 1\n",
    "        idx = str(cnt)\n",
    "        while len(idx) < 3:\n",
    "            idx = '0' + idx\n",
    "        \n",
    "        uk_text = ' '.join(UK[uk_book][uk_chap])#[:1000]\n",
    "        us_text = ' '.join(US[us_book][us_chap])#[:1000]\n",
    "        \n",
    "        # we add end-of-sentence (EOS) markers:\n",
    "        uk_text = '<EOS> '.join(sent_tokenize(uk_text))\n",
    "        us_text = '<EOS> '.join(sent_tokenize(us_text))\n",
    "        \n",
    "        uk, us = java_alignment(uk_text, us_text)\n",
    "        \n",
    "        fn = f\"collations/{idx}-{uk_book.replace(' ', '_')}-{uk_chap.replace(' ', '_')}.json\"\n",
    "        with open(fn, 'w') as f:\n",
    "            json.dump((uk, us), f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the collation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we already hinted at the possibility of establishing a simple categorization of the differences which we find between the UK and US series. Separating superficial orthographic conventions from deeper stylistic interventions, for instance, would be one clear step forward in trying to arrive at an empirical and quantitative characterization of the way in which both versions differ.\n",
    "\n",
    "#### Synonyms\n",
    "Reserving a category for \"plain\" synonyms, for instance, would be a relevant option. Here, one could wonder whether terms such as `pitch` and `field` are truly close enough to be considered semantically interchangable, but (as Philip Nel convincingly argues) we should not forget that such words do come with a different set of associations along both sides of the Ocean. One interesting resource to help up resolve such issues in a replicatable manner is **WordNet**. [Wordnet](https://wordnet.princeton.edu/) is a lexical resource from the fields of corpus linguistics and computational linguitics that provides a large database of words. In this gigantic tree structure, nouns, verbs, adjectives and adverbs are grouped into sets of synonyms that are called **synsets**. Each of these sets covers a distant concept and will hold, amongst other, the various synonyms that can be used to express this concept.\n",
    "\n",
    "Below, we define a simple function that interfaces WordNet through the NLTK. When fed two words, the function will simply return whether or not it considers these words synonyms as a boolean variable that will be `True` or `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def is_synonym(w1, w2):\n",
    "    w1 = w1.strip().lower()\n",
    "    w2 = w2.strip().lower()\n",
    "    synonym = False\n",
    "    for synset in wn.synsets(w1):\n",
    "        for lemma in synset.lemma_names():\n",
    "            if lemma.lower() == w2.lower():\n",
    "                synonym = True\n",
    "    for synset in wn.synsets(w2):\n",
    "        for lemma in synset.lemma_names():\n",
    "            if lemma.lower() == w1.lower():\n",
    "                synonym = True\n",
    "    return synonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we try on this function on a number of word pairs. (Feel free to add a couple of pairs of your own.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_synonym('torch', 'flashlight'))\n",
    "print(is_synonym('torch', 'dog'))\n",
    "print(is_synonym('cutting', 'clipping'))\n",
    "print(is_synonym('pitch', 'field'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that WordNet is large, but that its coverage of the English language is far from exhaustive or perfect. (We might therefore consider to expand this resource with a list of our own.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling differences\n",
    "\n",
    "Another straightforward category to \"catch\" are orthographic differences between the UK and US spellings that are used fairly consistently throughout the books. In the following code we define a function that attempts to decide whether two non-identical strings are in fact just spelling differences. We do so on the basis of a long list of UK and US spelling differences which we found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_to_us_spellings = []\n",
    "for line in open('uk_vs_us.txt', 'r'):\n",
    "    if not line.strip().startswith('#'):\n",
    "        uk_to_us_spellings.append(line.strip().split())\n",
    "\n",
    "def is_spelling_diff(w1, w2):\n",
    "    spelling = False\n",
    "    w1 = w1.strip().lower()\n",
    "    w2 = w2.strip().lower()\n",
    "    \n",
    "    if ''.join(OrderedDict.fromkeys(w1)) == ''.join(OrderedDict.fromkeys(w2)):\n",
    "        spelling = True\n",
    "    elif ''.join(w1.split()) == ''.join(w2.split()):\n",
    "        spelling = True\n",
    "    else:\n",
    "        for british, american in uk_to_us_spellings:\n",
    "            if w1.endswith(british) and w2.endswith(american):\n",
    "                spelling = True\n",
    "    \n",
    "    return spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to \"stress test\" the coverage of the function -- which seems pretty okay overall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_spelling_diff('towards', 'toward'))\n",
    "print(is_spelling_diff('favourite', 'favorite'))\n",
    "print(is_spelling_diff('analyse', 'analyze'))\n",
    "print(is_spelling_diff('dog', 'torch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function that allows us to \"score\" the differences between two collated witnesses. By this, we mean that we keep track of:\n",
    "\n",
    "1. the length of the UK chapter in characters (which we will use to normalize our counts)\n",
    "2. the number of spelling modifications\n",
    "3. the number of obvious synonym replacements\n",
    "4. the number of \"stylistic\" interventions, i.e. interventions which were included in the previous two categories\n",
    "5. the number of sentence boundaries introduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_diffs(uk, us, verbose=False):\n",
    "    diffs = {'len': len(uk_chap.strip()), # for normalization\n",
    "             'spell': 0, 'synon': 0, 'style': 0, 'eos': 0}\n",
    "\n",
    "    for a, b in zip(uk, us):\n",
    "        if not (a is None or b is None):\n",
    "            a = ''.join(a).strip().lower()\n",
    "            b = ''.join(b).strip().lower()\n",
    "            \n",
    "            a = ''.join([c for c in a if c.isalpha() or c.isspace() or c in ('<', '>')])\n",
    "            b = ''.join([c for c in b if c.isalpha() or c.isspace() or c in ('<', '>')])\n",
    "            \n",
    "            if a and b and a != b:\n",
    "                if not '<eos>' in a and '<eos>' in b:\n",
    "                    diffs['eos'] += 1\n",
    "                    if verbose:\n",
    "                        print(a, 'vs', b, '-> <EOS>')\n",
    "                elif is_spelling_diff(a, b):\n",
    "                    diffs['spell'] += 1\n",
    "                    if verbose:\n",
    "                        print(a, 'vs', b, '-> SPELLING')\n",
    "                elif is_synonym(a, b):\n",
    "                    diffs['synon'] += 1\n",
    "                    if verbose:\n",
    "                        print(a, 'vs', b, '-> SYNONYM')\n",
    "                else:\n",
    "                    diffs['style'] += 1\n",
    "                    if verbose:\n",
    "                        print(a, 'vs', b, '-> STYLISTIC')\n",
    "    \n",
    "    # normalize by the number of characters in the UK chapter\n",
    "    return diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a closer look as to how well our categorization performs. Do you notive any problems in the list below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_diffs(uk, us, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over the output of our collation and score each chapter pair. The results are stored in a so-called `pandas` **DataFrame**, a convenient way to store and query the data in a spreadsheet like fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['len', 'spell', 'synon', 'style', 'book', 'chapter', 'eos'])\n",
    "\n",
    "for fn in sorted(glob.glob('collations/*.json')):\n",
    "    with open(fn, 'r') as f:\n",
    "        uk, us = json.loads(f.read())\n",
    "    \n",
    "    score = score_diffs(uk, us, verbose=False)\n",
    "    score['book'] = os.path.basename(fn).split('-')[1]\n",
    "    score['chapter'] = os.path.basename(fn).split('-')[2]\n",
    "        \n",
    "    df = df.append(score, ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily export this dataframe to a comma-separated file format for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('align.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"style\" contains the number stylistic changes which we have captured. To normalize these absolute counts at the chapter level, we divide them by the number of characters in the chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('align.csv')\n",
    "df = df.infer_objects()\n",
    "df['norm'] = df['style'] / df['len']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the degree to which we find stylistic differences between the UK and US version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df['norm'].plot()\n",
    "ax.set_ylabel('stylistic changes / len')\n",
    "ax.set_xlabel('chapters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we just look at the series as a long list of chapters (which they essentially are). Some interesting patterns already emerge. However, we can also **aggregate** these scores at the book level, through taking the mean chapter score in each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby('book', sort=False)['norm'].mean().plot()\n",
    "ax.set_ylabel('mean stylistic delta')\n",
    "ax.set_xlabel('books')\n",
    "t = list(df['book'].unique())\n",
    "plt.xticks(range(len(t)), t, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the same pattern emerges: the degree of stylistic intervention in the first four books remains remarkably stable. It is again *The Phoenix* that breaks this trend and of all books it has clearly invited the highest number of stylistic edits. The final two books, on the other hand, have required much less edits than the fifth and we obtain even lower scores for them than for the first four books.\n",
    "\n",
    "These results could, in our opinion, be understood in two ways. Firstly, we could repeat the hypothesis that as the author's fame grew, she had more authority over her own text in the final two books; the editors might simply not have felt the need to intervene because they could expect that the readership would like the books anyway. Another hypothesis is that the author became more acutely aware of the scope of her American readership and simply started catering more for the US readership, already in the Uk version. As such, the decreased level of difference between both versions could also be seen as a conscious marketing/manufacturing strategy that eased the dissemination of the books across the global English market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do It Yourself\n",
    "\n",
    "> Align a random chapter from one of the last two books and visualize the alignment table in the notebook. Have a look at the differences which were found and make a list of items that were wrongly categorized in your eyes. Many \"stylistic\" changes, for instance, might in fact be simply spelling differences that weren't captured in our list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:clean36]",
   "language": "python",
   "name": "conda-env-clean36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
